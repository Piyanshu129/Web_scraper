# ğŸ“ Complete File Structure Explanation

This document explains every file in the project, what it does, and how they all work together.

---

## ğŸ—‚ï¸ Project Structure Overview

```
WEB_SCRAPER/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                      # â­ START HERE - Entry point
â”œâ”€â”€ ğŸ“„ requirements.txt             # Dependencies list
â”œâ”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ ğŸ“‚ src/                         # Main source code directory
â”‚   â”œâ”€â”€ __init__.py                 # Makes src a Python package
â”‚   â”œâ”€â”€ jira_client.py              # ğŸ”Œ API communication layer
â”‚   â”œâ”€â”€ state_manager.py            # ğŸ’¾ State persistence layer
â”‚   â”œâ”€â”€ data_transformer.py         # ğŸ”„ Data transformation layer
â”‚   â””â”€â”€ scraper.py                  # ğŸ¯ Main orchestration logic
â”‚
â”œâ”€â”€ ğŸ“‚ state/                       # Auto-generated: stores progress
â”‚   â””â”€â”€ scrape_state.json           # Current scraping state
â”‚
â”œâ”€â”€ ğŸ“‚ venv/                        # Virtual environment (auto-created)
â”‚
â”œâ”€â”€ ğŸ“„ jira_dataset.jsonl           # Output file (auto-generated)
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # Main documentation
â”œâ”€â”€ ğŸ“„ QUICKSTART.md                # Quick reference guide
â”‚
â””â”€â”€ ğŸ§ª Test & Utility Files
    â”œâ”€â”€ setup.py                    # Setup verification script
    â”œâ”€â”€ test_api_connection.py      # API connection test
    â”œâ”€â”€ test_specific_issue.py      # Single issue test
    â””â”€â”€ example_usage.py            # Usage examples
```

---

## ğŸ“‹ Detailed File Explanations

### ğŸ¯ **1. main.py** - The Entry Point

**Purpose**: Main entry point when you run the scraper from command line

**What it does**:
- Parses command-line arguments (`--projects`, `--output`, etc.)
- Sets up logging (saves to `scraper.log` and prints to console)
- Creates a `JiraScraper` instance
- Handles user commands (reset, start scraping)
- Prints summary statistics when done

**When it runs**: First file executed when you type `python main.py`

**Key Functions**:
```python
def main():
    # 1. Parse command line arguments
    # 2. Create JiraScraper instance
    # 3. Call scraper.scrape_all()
    # 4. Print results
```

**Example Usage**:
```bash
python main.py --projects SPARK HADOOP --delay 1.5
```

---

### ğŸ”Œ **2. src/jira_client.py** - API Communication Layer

**Purpose**: Handles all HTTP requests to Apache Jira REST API

**What it contains**:
- `JiraClient` class - Main API client
- Methods to fetch issues, comments, project info
- Error handling (429 rate limits, 5xx errors, timeouts)
- Retry logic with exponential backoff
- Rate limiting delays

**Key Methods**:
- `get_issue(issue_key)` - Fetch single issue details
- `search_issues(project, start_at, max_results)` - Search with pagination
- `get_issue_comments(issue_key)` - Get comments for an issue
- `get_project_info(project_key)` - Get project metadata
- `_make_request()` - Internal method with retry logic

**How it handles errors**:
- HTTP 429 (Rate Limiting) â†’ Waits and retries
- HTTP 5xx (Server Errors) â†’ Exponential backoff retry
- Timeout â†’ Retries with increasing delays
- Connection Error â†’ Retries up to 5 times

**Example Flow**:
```
User calls â†’ search_issues('SPARK', start_at=0)
           â†“
    _make_request('GET', 'search', params={...})
           â†“
    Makes HTTP request to Jira API
           â†“
    If error â†’ Retry with backoff
           â†“
    Returns JSON response
```

---

### ğŸ’¾ **3. src/state_manager.py** - State Persistence Layer

**Purpose**: Tracks scraping progress so you can resume after interruptions

**What it does**:
- Saves which issues have been processed
- Stores pagination position (start_at index)
- Allows resuming from last checkpoint
- Manages state file: `state/scrape_state.json`

**Key Methods**:
- `get_processed_issues(project)` - Get set of already-scraped issue keys
- `mark_issue_processed(project, issue_key)` - Mark issue as done
- `update_project_progress(project, start_at)` - Save pagination position
- `save_state()` - Write state to disk
- `reset_project(project)` - Clear state for a project

**State File Structure**:
```json
{
  "projects": {
    "SPARK": {
      "start_at": 150,           // Current pagination position
      "last_issue_key": "SPARK-12345"
    }
  },
  "processed_issues": {
    "SPARK": ["SPARK-1", "SPARK-2", ...],  // Already scraped
    "HADOOP": ["HADOOP-1", ...]
  },
  "last_updated": "2025-01-01T12:00:00"
}
```

**Why it's important**: 
- If scraping crashes, you can resume without re-scraping everything
- Tracks duplicates to avoid processing same issue twice

---

### ğŸ”„ **4. src/data_transformer.py** - Data Transformation Layer

**Purpose**: Converts raw Jira API responses into clean JSONL format for LLM training

**What it does**:
- Extracts plain text from HTML/structured fields
- Formats timestamps to readable dates
- Extracts labels, components, comments
- Generates derived tasks (summarization, classification, Q&A)
- Writes to JSONL file (one JSON object per line)

**Key Methods**:
- `extract_text_content(field)` - Converts HTML/structured data to plain text
- `extract_comments(issue, comments_data)` - Extracts and formats comments
- `transform_issue(issue, comments_data)` - Main transformation method
- `create_derived_tasks(issue_data)` - Generates LLM training tasks
- `write_jsonl(data, output_file)` - Writes to output file

**Transformation Flow**:
```
Raw Jira API JSON
     â†“
Extract fields (title, description, status, etc.)
     â†“
Clean HTML from description/comments
     â†“
Format timestamps
     â†“
Generate derived tasks
     â†“
Clean JSONL format
```

**Output Format**:
```json
{
  "issue_key": "SPARK-12345",
  "title": "Issue title",
  "description": "Clean plain text description",
  "status": "Resolved",
  "comments": [...],
  "derived_tasks": {
    "summarization": {...},
    "classification": {...},
    "qa_generation": {...}
  }
}
```

---

### ğŸ¯ **5. src/scraper.py** - Main Orchestration Logic

**Purpose**: Coordinates everything - the "conductor" of the orchestra

**What it contains**:
- `JiraScraper` class - Main scraper that ties everything together
- Orchestrates API calls, state management, and data transformation
- Handles pagination across multiple projects
- Manages batches and saves progress periodically

**Key Methods**:
- `__init__()` - Initializes all components (client, state, transformer)
- `scrape_project(project)` - Scrapes one project with pagination
- `scrape_all()` - Scrapes all configured projects
- `reset_project(project)` - Resets state for a project

**Execution Flow in `scrape_project()`**:
```
1. Check state for already-processed issues
2. Get total issue count from API
3. Loop through pages (pagination):
   a. Fetch page of issues from API
   b. For each issue:
      - Check if already processed â†’ skip
      - Fetch issue details + comments
      - Transform to JSONL format
      - Add to batch
      - Mark as processed
   c. Every N issues (batch_size):
      - Write batch to JSONL file
      - Save state to disk
4. Return total scraped count
```

**How Components Work Together**:
```python
JiraScraper
    â”œâ”€â”€ Uses JiraClient â†’ Makes API calls
    â”œâ”€â”€ Uses StateManager â†’ Tracks progress
    â””â”€â”€ Uses DataTransformer â†’ Converts data
```

---

### ğŸ“¦ **6. src/__init__.py** - Package Initializer

**Purpose**: Makes `src` directory a Python package

**What it does**: 
- Allows importing like: `from src.jira_client import JiraClient`
- Currently just contains a comment, but required for package structure

---

## ğŸ”„ Complete Execution Flow

Here's what happens when you run `python main.py`:

### Step-by-Step Flow:

```
1. main.py starts
   â†“
2. Parse command-line arguments
   â†“
3. Set up logging
   â†“
4. Create JiraScraper instance:
   â”œâ”€â”€ Creates JiraClient (API handler)
   â”œâ”€â”€ Creates StateManager (loads saved state)
   â””â”€â”€ Creates DataTransformer (data converter)
   â†“
5. Check for reset commands (if --reset flag)
   â†“
6. Call scraper.scrape_all()
   â†“
7. For each project:
   â”œâ”€â”€ scraper.scrape_project(project)
   â”‚   â”œâ”€â”€ Check StateManager: Which issues already processed?
   â”‚   â”œâ”€â”€ Call JiraClient.search_issues() â†’ Get page of issues
   â”‚   â”‚   â””â”€â”€ JiraClient handles retries, rate limits, errors
   â”‚   â”œâ”€â”€ For each issue in page:
   â”‚   â”‚   â”œâ”€â”€ Call JiraClient.get_issue() â†’ Get details
   â”‚   â”‚   â”œâ”€â”€ Call JiraClient.get_issue_comments() â†’ Get comments
   â”‚   â”‚   â”œâ”€â”€ Call DataTransformer.transform_issue() â†’ Convert to JSONL
   â”‚   â”‚   â”œâ”€â”€ StateManager.mark_issue_processed() â†’ Track progress
   â”‚   â”‚   â””â”€â”€ Add to batch
   â”‚   â”œâ”€â”€ Every batch_size issues:
   â”‚   â”‚   â”œâ”€â”€ DataTransformer.write_jsonl() â†’ Save to file
   â”‚   â”‚   â””â”€â”€ StateManager.save_state() â†’ Save progress
   â”‚   â””â”€â”€ Continue pagination until done
   â†“
8. Print summary statistics
   â†“
9. Done! âœ…
```

### Visual Flow Diagram:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   main.py   â”‚  â† You run this
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  JiraScraper     â”‚  â† Orchestrates everything
â”‚  (scraper.py)    â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
   â”‚           â”‚
   â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚JiraClientâ”‚ â”‚  StateManager    â”‚
â”‚(API calls)â”‚ â”‚(Progress tracking)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚                  â”‚
     â”‚                  â–¼
     â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚         â”‚ scrape_state.jsonâ”‚
     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DataTransformer    â”‚
â”‚ (Convert to JSONL) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ jira_dataset.jsonlâ”‚ â† Final output
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Supporting Files

### **requirements.txt**
- Lists all Python dependencies
- Used by `pip install -r requirements.txt`
- Contains: `requests`, `python-dateutil`, `tqdm`

### **.gitignore**
- Tells Git which files to ignore
- Ignores: `__pycache__/`, `venv/`, `*.jsonl`, `state/`, etc.

### **README.md**
- Comprehensive documentation
- Setup instructions, architecture, edge cases

### **QUICKSTART.md**
- Quick reference guide
- Basic usage examples

### **setup.py**
- Verifies installation
- Checks Python version, dependencies, project structure
- Run: `python setup.py`

### **test_api_connection.py**
- Tests if you can connect to Apache Jira API
- Useful for troubleshooting
- Run: `python test_api_connection.py`

### **test_specific_issue.py**
- Tests fetching a single issue
- Shows raw API response and transformed output
- Useful for debugging

### **example_usage.py**
- Shows programmatic usage examples
- How to use scraper in your own code

---

## ğŸ”— How Files Depend on Each Other

### Dependency Graph:

```
main.py
  â””â”€â”€â†’ scraper.py (JiraScraper class)
          â”œâ”€â”€â†’ jira_client.py (JiraClient class)
          â”œâ”€â”€â†’ state_manager.py (StateManager class)
          â””â”€â”€â†’ data_transformer.py (DataTransformer class)
```

### Import Chain:

```python
# main.py imports:
from src.scraper import JiraScraper

# scraper.py imports:
from .jira_client import JiraClient
from .state_manager import StateManager
from .data_transformer import DataTransformer

# No circular dependencies! âœ…
```

---

## ğŸ“Š Data Flow Example

Let's trace a single issue through the system:

### Example: Fetching issue "SPARK-12345"

1. **main.py**: User runs `python main.py --projects SPARK`
2. **scraper.py**: `scrape_project('SPARK')` called
3. **jira_client.py**: 
   - `search_issues('SPARK')` returns list including "SPARK-12345"
   - `get_issue('SPARK-12345')` fetches full details
   - `get_issue_comments('SPARK-12345')` fetches comments
4. **state_manager.py**: 
   - Checks if "SPARK-12345" in processed list (no)
   - Marks it as processed
5. **data_transformer.py**: 
   - `transform_issue()` converts raw JSON to clean format
   - Extracts text, formats dates, generates tasks
6. **data_transformer.py**: 
   - `write_jsonl()` appends to `jira_dataset.jsonl`
7. **state_manager.py**: 
   - `save_state()` updates `state/scrape_state.json`
8. **scraper.py**: Moves to next issue

---

## ğŸ¯ Quick Reference: What Each File Does

| File | Purpose | When It Runs |
|------|---------|--------------|
| `main.py` | Entry point, CLI interface | First (you run this) |
| `src/scraper.py` | Orchestrates scraping process | Called by main.py |
| `src/jira_client.py` | Makes API calls to Jira | Called by scraper.py |
| `src/state_manager.py` | Tracks progress | Called by scraper.py |
| `src/data_transformer.py` | Converts data format | Called by scraper.py |

---

## ğŸš€ Running the Scraper - What Happens

### Command: `python main.py --projects SPARK --delay 1.5`

1. **main.py** executes
   - Parses `--projects SPARK` and `--delay 1.5`
   - Sets up logging

2. **JiraScraper** initializes
   - Creates JiraClient with 1.5s delay
   - Creates StateManager, loads existing state
   - Creates DataTransformer

3. **Scraping begins**
   - For each page of issues:
     - JiraClient fetches issues
     - StateManager checks which are new
     - DataTransformer converts each issue
     - StateManager saves progress

4. **Results**
   - `jira_dataset.jsonl` - Contains all scraped issues
   - `state/scrape_state.json` - Contains progress
   - Console shows progress and summary

---

## ğŸ’¡ Key Concepts

### **Separation of Concerns**
- Each file has a single, clear responsibility
- Easy to test and maintain
- Can modify one layer without affecting others

### **Error Handling**
- JiraClient handles API errors
- StateManager handles file errors
- Each layer handles its own errors

### **Resumability**
- StateManager tracks progress
- Can stop and resume anytime
- No duplicate scraping

### **Batch Processing**
- Processes issues in batches
- Saves frequently (every N issues)
- Memory efficient

---

This architecture makes the code:
- âœ… **Maintainable** - Clear separation of concerns
- âœ… **Testable** - Each component can be tested independently
- âœ… **Resumable** - Can restart after interruptions
- âœ… **Scalable** - Can handle large datasets efficiently
- âœ… **Robust** - Comprehensive error handling

